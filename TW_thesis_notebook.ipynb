{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bieropener/TW_thesis_Marsman/blob/main/TW_thesis_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WK9gZG8Ouivi"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stP6VTuSuVrk"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "# install the trained Dutch pipeline, used for tokenizing the corpus\n",
        "!python -m spacy download nl_core_news_sm\n",
        "!pip install lxml\n",
        "# clone the repository of OpenDutchWordnet, the dictionary\n",
        "!rm -rf OpenDutchWordnet\n",
        "!git clone https://github.com/cltl/OpenDutchWordnet.git\n",
        "!python setup.py install\n",
        "!pip install rdflib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWD_qL_hvLrP"
      },
      "source": [
        "Import all tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yeo8pd5jvP7Y"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "# Dutch pipeline for tokenizing the corpus\n",
        "import spacy\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "# Lxml to deal with XML-type files\n",
        "import lxml\n",
        "# Open Dutch WordNet for dictionary meanings\n",
        "from OpenDutchWordnet import Wn_grid_parser\n",
        "odwn = Wn_grid_parser(Wn_grid_parser.odwn)\n",
        "# BERTje model for contextualized word embeddings\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
        "model = AutoModel.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\n",
        "# tools to compare word embeddings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8I2hwkMw0iO"
      },
      "source": [
        "Function definitions for preprocessing Lassy Small corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfBdKpOCvrfv"
      },
      "outputs": [],
      "source": [
        "# takes a XML-file and then extracts the plain text sentence from that file\n",
        "def Lassy_XML_to_text(file):\n",
        "  # create empty list to add all sentences to\n",
        "  sents = []\n",
        "  # read the file\n",
        "  tree = ET.parse(file)\n",
        "  # get the root element of the tree, which contains all other elements\n",
        "  root = tree.getroot()\n",
        "  # find every <sentence> element and add its text to the list of sentences\n",
        "  for sent in root.iter('sentence'):\n",
        "    sents.append(sent.text)\n",
        "  return sents\n",
        "\n",
        "# takes a list of strings with XML filenames, returns a text with plain text from these files\n",
        "def Lassy_filelist_to_text(file_list):\n",
        "  # create list for all sentences from different files\n",
        "  all_sentences = []\n",
        "  # extract the plain text by iterating over these files\n",
        "  for filename in file_list:\n",
        "    new_sent = Lassy_XML_to_text(filename)\n",
        "    for word in new_sent:\n",
        "      all_sentences.append(word)\n",
        "  # Join list of all sentences together to form one text.\n",
        "  one_text = ' '.join(all_sentences)\n",
        "  return one_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhMwxdOVyb7g"
      },
      "source": [
        "Function definitions for retrieving and comparing basic and contextual meaning for lexical units from a corpus of plain text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNH6hGKLyg-C"
      },
      "outputs": [],
      "source": [
        "# Function to get basic meanings for all nouns in a tokenized text.\n",
        "def get_basic_meanings(corpus):\n",
        "  basic_meanings = {}\n",
        "  for token in corpus:\n",
        "    if token.pos_ == \"NOUN\":\n",
        "      meanings_num = odwn.lemma_num_senses(token.text, pos='noun')\n",
        "      if meanings_num >1:\n",
        "        token_entry = odwn.les_find_le(f\"{token.text}-n-1\")\n",
        "        if token_entry:\n",
        "          token_def = token_entry.get_definition()\n",
        "          basic_meanings[token] = token_def\n",
        "  return basic_meanings\n",
        "\n",
        "# Function to get the contextual embedding of a token using BERTje.\n",
        "def get_contextual_embedding(LU, sent):\n",
        "  inputs = tokenizer(sent, return_tensors = \"pt\", padding = True, truncation = True)\n",
        "  outputs = model(**inputs)\n",
        "  tokenized_sentence = tokenizer.tokenize(sent)\n",
        "  LU_tokens = tokenizer.tokenize(LU.text)\n",
        "  LU_index = -1\n",
        "  for i in range(len(tokenized_sentence)-  len(LU_tokens)+1):\n",
        "    if tokenized_sentence[i:i + len(LU_tokens)] == LU_tokens:\n",
        "      LU_index = i\n",
        "      break\n",
        "  if LU_index == -1:\n",
        "    raise ValueError(f\"Could not find tokens for {LU.text} in the tokenized sentence\")\n",
        "  #get index of token in tokenized input\n",
        "  # LU_index = inputs.input_ids[0].tolist().index(tokenizer.convert_tokens_to_ids(LU.text))\n",
        "  #get contextualized embedding\n",
        "  LU_embedding = outputs.last_hidden_state[0,LU_index, :]\n",
        "  return LU_embedding\n",
        "\n",
        "# Function to get the synset embeddings for a token.\n",
        "def get_synset_embeddings(LU):\n",
        "  # create list for synsets\n",
        "  synsets = []\n",
        "  meanings_num = odwn.lemma_num_senses(LU.text, pos='noun')\n",
        "  if meanings_num >0:\n",
        "    for sense_num in range(1,meanings_num +1):\n",
        "      le = odwn.les_find_le(f\"{LU.text}-n-{sense_num}\")\n",
        "      if le:\n",
        "        synset_text = le.get_definition()\n",
        "        inputs = tokenizer(synset_text, return_tensors=\"pt\", padding = True, truncation = True)\n",
        "        outputs = model(**inputs)\n",
        "        synset_embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "        synsets.append((le, synset_embedding))\n",
        "    return synsets\n",
        "\n",
        "# function that retrieves contextual meanings for noun tokens in a dictionary, using the corpus for context\n",
        "def get_contextual_meanings(corpus, basic_dict):\n",
        "  con_meanings = {}\n",
        "  for token in basic_dict:\n",
        "    sentence = token.sent.text\n",
        "    word_embedding = get_contextual_embedding(token, sentence)\n",
        "    synsets = get_synset_embeddings(token)\n",
        "    if synsets:\n",
        "      best_synset = None\n",
        "      max_similarity = 0\n",
        "      for le, synset_embedding in synsets:\n",
        "        similarity = cosine_similarity(word_embedding.detach().numpy().reshape(1,-1), synset_embedding.detach().numpy().reshape(1,-1))[0][0]\n",
        "        if similarity > max_similarity:\n",
        "          max_similarity = similarity\n",
        "          best_synset = le\n",
        "        if best_synset:\n",
        "          con_meanings[token] = best_synset.get_definition()\n",
        "  return con_meanings\n",
        "\n",
        "def compare_meanings(basic_dict, contextual_dict):\n",
        "  non_literal_dict = {}\n",
        "  for word, meaning in basic_dict.items():\n",
        "    #if word in contextual_dict:\n",
        "    if meaning != contextual_dict[word]:\n",
        "      non_literal_dict[word] = (meaning, contextual_dict[word])\n",
        "  return non_literal_dict\n",
        "\n",
        "def preprocess(text):\n",
        "  lower_text = text.lower()\n",
        "  processed_text = nlp(lower_text)\n",
        "  no_punct_text = [token for token in processed_text if not token.is_punct]\n",
        "  no_num_text = [token for token in no_punct_text if not token.is_digit]\n",
        "  no_mrw_text = [token for token in no_num_text if token.text!=\"mrw\"]\n",
        "  return no_mrw_text\n",
        "\n",
        "def MIPVU(corpus):\n",
        "  processed_corpus = preprocess(corpus)\n",
        "  bdef = get_basic_meanings(processed_corpus)\n",
        "  cdef= get_contextual_meanings(processed_corpus, bdef)\n",
        "  res = compare_meanings(bdef, cdef)\n",
        "  return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USh00Bw61-H6"
      },
      "source": [
        "Import corpus and preprocess if necessary (for Lassy Small corpus). Then apply MIPVU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFOyoIGQAbOk"
      },
      "outputs": [],
      "source": [
        "# take a small set of the corpus, save it as a list of filenames\n",
        "file_list = ['WR-P-E-E-0000000018.p.1.s.1.xml','WR-P-E-E-0000000018.p.2.s.1.xml','WR-P-E-E-0000000018.p.2.s.2.xml','WR-P-E-E-0000000018.p.2.s.3.xml','WR-P-E-E-0000000018.p.2.s.4.xml','WR-P-E-E-0000000018.p.2.s.5.xml','WR-P-E-E-0000000018.p.3.s.1.xml','WR-P-E-E-0000000018.p.4.s.1.xml','WR-P-E-E-0000000018.p.5.s.1.xml','WR-P-E-E-0000000018.p.6.s.1.xml','WR-P-E-E-0000000018.p.7.s.1.xml','WR-P-E-E-0000000018.p.7.s.2.xml','WR-P-E-E-0000000018.p.7.s.3.xml','WR-P-E-E-0000000018.p.7.s.4.xml','WR-P-E-E-0000000018.p.7.s.5.xml','WR-P-E-E-0000000018.p.7.s.6.xml','WR-P-E-E-0000000018.p.7.s.7.xml','WR-P-E-E-0000000018.p.8.s.1.xml','WR-P-E-E-0000000018.p.8.s.2.xml','WR-P-E-E-0000000018.p.9.s.1.xml','WR-P-E-E-0000000018.p.10.s.1.xml','WR-P-E-E-0000000018.p.10.s.2.xml','WR-P-E-E-0000000018.p.10.s.3.xml','WR-P-E-E-0000000018.p.10.s.4.xml','WR-P-E-E-0000000018.p.10.s.5.xml','WR-P-E-E-0000000018.p.11.s.1.xml','WR-P-E-E-0000000018.p.11.s.2.xml','WR-P-E-E-0000000018.p.12.s.1.xml','WR-P-E-E-0000000018.p.13.s.1.xml','WR-P-E-E-0000000018.p.14.s.1.xml','WR-P-E-E-0000000018.p.14.s.2.xml','WR-P-E-E-0000000018.p.14.s.3.xml','WR-P-E-E-0000000018.p.14.s.4.xml','WR-P-E-E-0000000018.p.14.s.5.xml','WR-P-E-E-0000000018.p.14.s.6.xml','WR-P-E-E-0000000018.p.14.s.7.xml','WR-P-E-E-0000000018.p.15.s.1.xml','WR-P-E-E-0000000018.p.15.s.2.xml']\n",
        "corpus_Lassy = Lassy_filelist_to_text(file_list)\n",
        "# make sure the files in the file list together form a text and not more or less.\n",
        "print(corpus_Lassy)\n",
        "Lassy_MIPVU = MIPVU(corpus_Lassy)\n",
        "for word, meaning in Lassy_MIPVU.items():\n",
        "  print(word)\n",
        "  print(meaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqc6QoKeA_Mt"
      },
      "source": [
        "Try it out with some sample sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDz-BmBY2CAa"
      },
      "outputs": [],
      "source": [
        "test = 'De student moest een berg huiswerk maken in een weekend.'\n",
        "haha = MIPVU(test)\n",
        "for word, meaning in haha.items():\n",
        "  print(word)\n",
        "  print(meaning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pmxvNBF-sI4"
      },
      "source": [
        "Evaluation with Bruggen corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W0qOZIQi-0jh"
      },
      "outputs": [],
      "source": [
        "# read the file and preprocess the text\n",
        "Bruggen_file = open('Bruggen (2024) bijlage 1.txt', 'r')\n",
        "Bruggen_corpus = Bruggen_file.read()\n",
        "Bruggen_MIPVU = MIPVU(Bruggen_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMOn93GsTxWq"
      },
      "outputs": [],
      "source": [
        "for word, meaning in Bruggen_MIPVU.items():\n",
        "  print(f\"word: {word}, meaning: {meaning}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTk2P/ifHSeRx904k8jIXE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}